---
published: true
layout: page
permalink: /GSOC1/
---
Firstly, I would ask you to watch this great talk from Simon Sinek, I am waiting for you:<br/>

[![IMAGE ALT TEXT HERE](https://pi.tedcdn.com/r/pe.tedcdn.com/images/ted/04916ee6e81065c8333e6546184af512eee37bbe_2880x1620.jpg)](https://embed.ted.com/talks/simon_sinek_how_great_leaders_inspire_action){:target="_blank"}

So let's start with why;
## Why?
I come from an engineering background, with a specialization in mechatronics and robotics.<br/>
Two years ago, I was working on my graduation project, where I had to control a robotic arm to achieve standard tasks inside its workspace. At that time, we have manufactured a custom robotic arm by using laser cutting machines. It worked at the end, for the first milestone, we can send commands to the motors and it moves.<br/>
The next step is to achieve a task using our robotic arm, in the theory of the robotic control, we need the dynamic model of our robot. Depending on the dynamic model we can build the needed control system to achieve the tasks, but in our situation, the errors and approximation in the dynamic model affected very bad, so the results were unsatisfying.<br/>
We have a problem, and to solve it, we need an algorithm that can handle such approximations and errors. 

## How?
How to solve the problem, we have read about the possible solution, but the results of most of the past work show that it hard to make something like that work with a robot.<br/>
But one day, in May 2017. I was watching Google I/O 2017, there was a lecture about using a deep neural network to map directly from images to labels. Here, I had thought, why we can't use that in robotic, a deep neural network that maps between motor torques and the end effector position.<br/>
The previous notice, led me to research about such methods, and to find that, this idea already a hot research topic, and it is called "**Reinforcement learning**".<br/>
It was the starting point for my research journey, I have worked firstly with model-free algorithms like DDPG, TRPO, PPO. But in robotics, it is impractical to let the robot work for weeks to collect the big datasets which are needed to train deep neural networks. here I have decided to work with "**data-efficient reinforcement learning**"

## What?
Using probabilistic models of the environment could lead to better data efficiency. That fact has been visited when using Gaussian processes in model-based RL ([PILCO](https://arxiv.org/abs/1502.02860)), and when using Bayesian Neural Networks ([Deep PILCO](http://mlg.eng.cam.ac.uk/yarin/PDFs/DeepPILCO.pdf)). After working with such ideas, I have understood how the work with probabilistic models could be beneficial to produce practical robot learning algorithms.<br\>
So we can say, that the probabilistic models will be the tool for the future of robot learning, and that led me to choose to get hands-on training with them, so I have chosen to work with Gaussian processes, and as a user to TensorFlow, I will try not just to use Gaussian processes, but also to implement **Deep Gaussian Processes** in TF, to prepare it as a tool for futuristic research.<br\>
And now I have received a great opportunity to work on that, while receiving guidance from an experienced mentor, as a part of the **Google Summer of Code 2019**.<br\>

## What I want to say;
If you search about GSoC, you'll find a lot of blogs telling you how to crack it, how to improve chances in getting accepted, how to craft your proposal and to contribute just to be known from the organization.<br\>
I have got introduced to GSoC just 5 days before the deadline, I have written just one proposal, and get selected, all of them because I **have started with the WHY**, not with the **WHAT** (which is in our case GSoC), I will work during this summer, and after finishing I will continue to get hands-on experience with Bayesian neural networks, and Neural processes and try to contribute to Tensorflow when I get something valuable, not just to contribute.<br\>
I hope it would be useful for the students in the following years, and to promote those thoughts over the community of GSoC to get better results in the future.
